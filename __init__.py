import os
import json
import time
import logging
import threading
import boto3
import requests
import numpy as np
from PIL import Image
import io
import datetime
from botocore.exceptions import ClientError, NoCredentialsError, PartialCredentialsError
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type

# --- Setup a consistent logger for all nodes in this package ---
logger = logging.getLogger("ImagexNodes")
if not logger.handlers:
    handler = logging.StreamHandler()
    formatter = logging.Formatter('%(asctime)s - %(levelname)s - [%(name)s] - %(message)s', '%Y-%m-%d %H:%M:%S')
    handler.setFormatter(formatter)
    logger.addHandler(handler)
    logger.setLevel(logging.INFO)

# =================================================================================
# == S3 UPLOADER NODE
# =================================================================================
class ImagexS3Uploader:
    """
    Uploads an image to a pre-determined S3 path.

    This node is designed for security and integration. It requires the full S3 object key
    to be provided, which should be generated by the orchestrating backend (Imagex).
    Authentication relies on secure methods like IAM Roles or environment variables.
    """
    @classmethod
    def INPUT_TYPES(cls):
        """Defines the input types for the node."""
        return {
            "required": {
                "images": ("IMAGE",),
                "bucket_name": ("STRING", {"multiline": False}),
                "full_object_key": ("STRING", {"multiline": False}),
                "region_name": ("STRING", {"multiline": False, "default": "us-east-2"}),
            }
        }

    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("s3_url",)
    FUNCTION = "upload_to_s3"
    OUTPUT_NODE = True
    CATEGORY = "Imagex/AWS"

    def upload_to_s3(self, images, bucket_name, full_object_key, region_name="us-east-2"):
        """
        Uploads the provided image to the specified S3 bucket and key.

        Args:
            images (torch.Tensor): The image tensor batch from ComfyUI.
            bucket_name (str): The name of the target S3 bucket.
            full_object_key (str): The complete path and filename for the object in S3.
            region_name (str): The AWS region of the S3 bucket.

        Returns:
            A tuple containing the S3 URL of the uploaded object.
        """
        if not bucket_name or not full_object_key:
            logger.error("S3 Upload: Bucket name or full_object_key is missing. Skipping upload.")
            return ("",)

        try:
            s3_client = boto3.client("s3", region_name=region_name)
        except (NoCredentialsError, PartialCredentialsError) as e:
            logger.error(f"S3 Upload Error: AWS credentials not found. Configure them via IAM roles or environment variables. Details: {e}")
            return ("",)
        except Exception as e:
            logger.error(f"S3 Upload Error: Failed to create S3 client. Error: {e}", exc_info=True)
            return ("",)

        # Process only the first image from the batch for a 1-to-1 job-to-asset workflow
        image_tensor = images[0]
        image_np = image_tensor.cpu().numpy()
        image_pil = Image.fromarray((image_np * 255).astype(np.uint8))

        buffer = io.BytesIO()
        image_pil.save(buffer, format="PNG")
        buffer.seek(0)

        s3_url = f"s3://{bucket_name}/{full_object_key}"

        try:
            s3_client.upload_fileobj(buffer, bucket_name, full_object_key)
            logger.info(f"Successfully uploaded to {s3_url}")
            return (s3_url,)
        except ClientError as e:
            logger.error(f"S3 Upload Error: Failed to upload to {s3_url}. Error: {e}", exc_info=True)
            return ("",)
        except Exception as e:
            logger.error(f"An unexpected error occurred during S3 upload: {e}", exc_info=True)
            return ("",)

# =================================================================================
# == JOB COMPLETION NOTIFIER NODE
# =================================================================================
class ImagexJobCompleteNotifier:
    """
    Notifies an SQS queue that a job has been successfully completed.

    This node should be placed after the S3Uploader node in a workflow. It constructs
    a structured JSON message and sends it to the job completion SQS queue.
    """
    @classmethod
    def INPUT_TYPES(cls):
        """Defines the input types for the node."""
        return {
            "required": {
                "s3_url": ("STRING",), # Receive output from the S3Uploader
                "job_id": ("STRING", {"multiline": False}),
                "completion_queue_url": ("STRING", {"multiline": False}),
                "region_name": ("STRING", {"multiline": False, "default": "us-east-2"}),
            }
        }

    RETURN_TYPES = ()
    FUNCTION = "notify_completion"
    OUTPUT_NODE = True
    CATEGORY = "Imagex/AWS"

    def notify_completion(self, s3_url, job_id, completion_queue_url, region_name):
        """
        Sends a structured completion message to the specified SQS queue.

        Args:
            s3_url (str): The S3 URL of the generated asset.
            job_id (str): The unique identifier for the job.
            completion_queue_url (str): The URL of the SQS completion queue.
            region_name (str): The AWS region of the SQS queue.
        """
        if not all([job_id, s3_url, completion_queue_url]):
            logger.error("Job Notifier: Missing required inputs. Skipping notification.")
            return {}

        try:
            sqs_client = boto3.client('sqs', region_name=region_name)
        except Exception as e:
            logger.error(f"Job Notifier Error: Could not create SQS client. Error: {e}", exc_info=True)
            return {}

        try:
            bucket_name, key = s3_url.replace("s3://", "").split("/", 1)
            https_url = f"https://{bucket_name}.s3.{region_name}.amazonaws.com/{key}"
        except ValueError:
            logger.warning(f"Job Notifier: Could not parse S3 URL '{s3_url}'. Using it as is.")
            https_url = s3_url

        message_body = {
            "jobId": job_id,
            "status": "COMPLETED",
            "imageUrl": https_url,
            "thumbnailUrl": "", # To be populated later by the thumbnail Lambda
            "completedAt": datetime.datetime.now(datetime.timezone.utc).isoformat()
        }

        try:
            sqs_client.send_message(
                QueueUrl=completion_queue_url,
                MessageBody=json.dumps(message_body),
                MessageGroupId=job_id # Necessary for SQS FIFO queues
            )
            logger.info(f"Successfully sent completion notification for Job ID: {job_id}")
        except ClientError as e:
            logger.error(f"Job Notifier Error: Failed to send message to SQS. Error: {e}", exc_info=True)

        return {}

# =================================================================================
# == SQS WORKER AND LAUNCHER
# =================================================================================
# The SQSWorker class and its startup logic are included here.
# This entire section manages the background process that pulls jobs from SQS.

class SQSWorker:
    """
    A resilient and scalable worker that processes jobs from an SQS queue
    and submits them to the local ComfyUI instance.
    """
    def __init__(self, queue_url, region_name, comfyui_url, poll_wait_time, max_messages):
        self.queue_url = queue_url
        self.region_name = region_name
        self.comfyui_url = f"{comfyui_url.rstrip('/')}/prompt"
        self.poll_wait_time = poll_wait_time
        self.max_messages = max_messages
        self.sqs = boto3.client("sqs", region_name=self.region_name)
        self.session = requests.Session()
        logger.info(f"Worker initialized. Queue: {self.queue_url}, ComfyUI API: {self.comfyui_url}")

    def run(self):
        """Starts the main loop to poll the SQS queue."""
        logger.info("Starting SQS queue listener...")
        while True:
            try:
                logger.debug(f"Polling for messages from: {self.queue_url}")
                response = self.sqs.receive_message(
                    QueueUrl=self.queue_url,
                    MaxNumberOfMessages=self.max_messages,
                    WaitTimeSeconds=self.poll_wait_time,
                    AttributeNames=['ApproximateReceiveCount']
                )
                messages = response.get("Messages", [])
                if not messages:
                    continue
                for message in messages:
                    self._process_message(message)
            except ClientError as e:
                logger.error(f"AWS ClientError while connecting to SQS: {e}", exc_info=True)
                time.sleep(10)
            except Exception as e:
                logger.critical(f"An unexpected error occurred in the main loop: {e}", exc_info=True)
                time.sleep(15)

    def _process_message(self, message):
        """Processes a single structured SQS message."""
        receipt_handle = message["ReceiptHandle"]
        message_id = message["MessageId"]
        logger.info(f"Received message ID: {message_id}")
        try:
            self.sqs.change_message_visibility(
                QueueUrl=self.queue_url, ReceiptHandle=receipt_handle, VisibilityTimeout=300
            )
        except ClientError as e:
            logger.warning(f"Could not extend visibility timeout for {message_id}: {e}")
        try:
            message_envelope = json.loads(message["Body"])
            if "metadata" not in message_envelope or "payload" not in message_envelope:
                raise ValueError("SQS message does not contain 'metadata' or 'payload' fields.")
            metadata = message_envelope["metadata"]
            job_id = metadata.get("jobId", "N/A")
            logger.info(f"Processing Job ID: {job_id}")
            workflow_payload_str = message_envelope["payload"]
            self._submit_to_comfyui(workflow_payload_str)
            self.sqs.delete_message(QueueUrl=self.queue_url, ReceiptHandle=receipt_handle)
            logger.info(f"Job {job_id} submitted successfully. Deleted message {message_id}.")
        except (ValueError, json.JSONDecodeError, KeyError) as e:
            logger.error(f"Invalid message format for {message_id}: {e}")
            self.sqs.delete_message(QueueUrl=self.queue_url, ReceiptHandle=receipt_handle)
            logger.warning(f"Deleted invalid message {message_id} to prevent queue poisoning.")
        except Exception as e:
            logger.error(f"Failed to process message {message_id}. It will be reprocessed after timeout. Error: {e}", exc_info=True)

    @retry(
        stop=stop_after_attempt(5),
        wait=wait_exponential(multiplier=1, min=2, max=30),
        retry=retry_if_exception_type(requests.exceptions.RequestException)
    )
    def _submit_to_comfyui(self, workflow_json_str):
        """Submits the workflow prompt to the ComfyUI API with retries."""
        try:
            prompt_data = json.loads(workflow_json_str)
            payload = {"prompt": prompt_data}
            logger.info(f"Submitting prompt to ComfyUI API: {self.comfyui_url}")
            response = self.session.post(self.comfyui_url, json=payload, timeout=60)
            response.raise_for_status()
            logger.info(f"Successfully submitted prompt. API response: {response.status_code}")
            return response.json()
        except json.JSONDecodeError as e:
            logger.error(f"Error: The workflow payload is not valid JSON. Content: {workflow_json_str}")
            raise ValueError(f"Invalid workflow JSON: {e}") from e
        except requests.exceptions.RequestException as e:
            logger.warning(f"Could not connect to ComfyUI: {e}. Retrying...")
            raise

class ImagexSQSWorkerLauncherNode:
    """
    A utility node to display the status of the background SQS worker.
    The worker itself is started automatically when this node is loaded.
    """
    @classmethod
    def INPUT_TYPES(cls):
        return {"required": {"check_status": (["check_status"],)}}

    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("worker_status",)
    FUNCTION = "get_status"
    OUTPUT_NODE = True
    CATEGORY = "Imagex/AWS"

    def get_status(self, check_status):
        """Returns the current operational status of the background worker thread."""
        is_running = SQS_WORKER_THREAD is not None and SQS_WORKER_THREAD.is_alive()
        status_message = f"SQS Worker Status: {'RUNNING' if is_running else 'STOPPED'}\nQueue: {SQS_QUEUE_URL or 'Not Configured'}"
        return (status_message,)

# --- Background Thread Startup Logic ---
# This code runs once when ComfyUI loads the custom nodes.
SQS_WORKER_THREAD = None
SQS_QUEUE_URL = os.getenv("SQS_QUEUE_URL")

def start_worker_in_background():
    """Configures and starts the SQSWorker in a separate daemon thread."""
    region = os.getenv("AWS_REGION", "us-east-2")
    comfyui_url = os.getenv("COMFYUI_URL", "http://127.0.0.1:8188")
    poll_wait_time = int(os.getenv("SQS_POLL_WAIT_TIME", 20))
    max_messages = int(os.getenv("SQS_MAX_MESSAGES", 1))

    worker = SQSWorker(
        queue_url=SQS_QUEUE_URL,
        region_name=region,
        comfyui_url=comfyui_url,
        poll_wait_time=poll_wait_time,
        max_messages=max_messages
    )
    worker.run()

if SQS_QUEUE_URL:
    logger.info("SQS_QUEUE_URL found. Initializing SQS worker in a background thread.")
    SQS_WORKER_THREAD = threading.Thread(target=start_worker_in_background)
    SQS_WORKER_THREAD.daemon = True # This is crucial to allow ComfyUI to exit gracefully
    SQS_WORKER_THREAD.start()
    logger.info("Imagex SQS Worker thread has been started.")
else:
    logger.warning("SQS_QUEUE_URL environment variable not set. The SQS worker will NOT start.")

# =================================================================================
# == NODE REGISTRATION
# =================================================================================
NODE_CLASS_MAPPINGS = {
    "ImagexS3Uploader": ImagexS3Uploader,
    "ImagexJobCompleteNotifier": ImagexJobCompleteNotifier,
    "ImagexSQSWorkerLauncher": ImagexSQSWorkerLauncherNode
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "ImagexS3Uploader": "Imagex S3 Uploader",
    "ImagexJobCompleteNotifier": "Imagex Job Completion Notifier",
    "ImagexSQSWorkerLauncher": "Imagex SQS Worker Status"
}
